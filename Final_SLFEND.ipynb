{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMpUmmuWLpf0qv2l8oLYVQQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amirgh8080/My-SLFEND/blob/main/Final_SLFEND.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp3vlcmFYYNt",
        "outputId": "6224bbaf-aa42-46a1-b396-ea04ad46a5da"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.0818, Acc: 0.9761, Val Loss: 0.0008, Val Acc: 1.0000\n",
            "Epoch 2/10, Loss: 0.0063, Acc: 0.9989, Val Loss: 0.0003, Val Acc: 1.0000\n",
            "Epoch 3/10, Loss: 0.0012, Acc: 0.9997, Val Loss: 0.0002, Val Acc: 1.0000\n",
            "Epoch 4/10, Loss: 0.0001, Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
            "Epoch 5/10, Loss: 0.0000, Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
            "Epoch 6/10, Loss: 0.0000, Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
            "Epoch 7/10, Loss: 0.0000, Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
            "Epoch 8/10, Loss: 0.0000, Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
            "Epoch 9/10, Loss: 0.0000, Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n",
            "Epoch 10/10, Loss: 0.0000, Acc: 1.0000, Val Loss: 0.0000, Val Acc: 1.0000\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# --- Data Loading and Preprocessing ---\n",
        "def load_csv(file_path):\n",
        "    \"\"\"Load dataset CSV.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "def preprocess_data(df, text_column, label_column):\n",
        "    \"\"\"Preprocess data, converting text and labels to lists.\"\"\"\n",
        "    df = df.dropna(subset=[text_column, label_column])\n",
        "    df[label_column] = df[label_column].apply(lambda x: 1 if x == 'Real' else 0)\n",
        "    texts = df[text_column].tolist()\n",
        "    labels = df[label_column].tolist()\n",
        "    return texts, labels\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    \"\"\"Custom Dataset class for news data.\"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
        "            'label': torch.tensor(label, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# --- Model Architecture ---\n",
        "class BertEmbedding(nn.Module):\n",
        "    \"\"\"BERT encoder to extract textual embeddings.\"\"\"\n",
        "    def __init__(self, bert_model_name='bert-base-uncased'):\n",
        "        super(BertEmbedding, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return outputs.last_hidden_state  # Return hidden states [batch_size, seq_len, hidden_size]\n",
        "\n",
        "class LeapGRU(nn.Module):\n",
        "    \"\"\"Leap GRU module for skipping irrelevant words.\"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LeapGRU, self).__init__()\n",
        "        self.gru = nn.GRUCell(input_size, hidden_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2 + input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 2),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        h = torch.zeros(batch_size, self.hidden_size).to(x.device)\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            ht = self.gru(x[:, t, :], h)\n",
        "            context = torch.cat([h, ht, x[:, t, :]], dim=-1)\n",
        "            skip_prob = self.mlp(context)[:, 1]  # Skip probability based on context\n",
        "\n",
        "            if skip_prob.mean() >= 0.5:\n",
        "                h = ht  # Update hidden state only if skip_prob is high\n",
        "\n",
        "            outputs.append(h)\n",
        "\n",
        "        outputs = torch.stack(outputs, dim=1)\n",
        "        return outputs  # [batch_size, seq_len, hidden_size]\n",
        "\n",
        "class MembershipFunction(nn.Module):\n",
        "    \"\"\"Generates soft labels for multi-domain fake news detection.\"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MembershipFunction, self).__init__()\n",
        "        self.leap_gru = LeapGRU(input_size, hidden_size)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 9),  # 9 domain labels (based on the paper)\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        leap_output = self.leap_gru(x)\n",
        "        h = leap_output[:, -1, :]  # Final hidden state\n",
        "        soft_labels = self.mlp(h)\n",
        "        return soft_labels  # Soft labels [batch_size, 9]\n",
        "\n",
        "class TextCNN(nn.Module):\n",
        "    \"\"\"Text CNN for feature extraction.\"\"\"\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(TextCNN, self).__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, 100, (k, input_size)) for k in [3, 4, 5]\n",
        "        ])\n",
        "        self.fc = nn.Linear(300, num_classes)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # [batch_size, 1, seq_len, input_size]\n",
        "        conv_results = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
        "        pooled = [F.max_pool1d(result, result.size(2)).squeeze(2) for result in conv_results]\n",
        "        cat = torch.cat(pooled, dim=1)\n",
        "        out = self.dropout(self.fc(cat))\n",
        "        return out  # [batch_size, num_classes]\n",
        "\n",
        "class DomainGate(nn.Module):\n",
        "    \"\"\"Applies domain gate logic to weigh experts' outputs.\"\"\"\n",
        "    def __init__(self, input_size):\n",
        "        super(DomainGate, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 9),  # 9 domains\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, g):\n",
        "        alpha = self.mlp(g)\n",
        "        return alpha  # Soft domain weights [batch_size, 9]\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    \"\"\"Final classifier for detecting fake news.\"\"\"\n",
        "    def __init__(self, input_size):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_size, 384),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(384, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, v):\n",
        "        return self.mlp(v).squeeze(-1)  # Output fake/real label [batch_size]\n",
        "\n",
        "class SLFENDModel(nn.Module):\n",
        "    \"\"\"Soft-label multi-domain fake news detection (SLFEND) model.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(SLFENDModel, self).__init__()\n",
        "        self.bert = BertEmbedding()\n",
        "        self.membership_function = MembershipFunction(input_size=768, hidden_size=256)\n",
        "        self.experts = nn.ModuleList([TextCNN(768, 128) for _ in range(9)])  # 9 expert networks\n",
        "        self.domain_gate = DomainGate(9)\n",
        "        self.classifier = Classifier(128)  # Classify final weighted output\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        bert_output = self.bert(input_ids, attention_mask)  # [batch_size, seq_len, 768]\n",
        "        soft_labels = self.membership_function(bert_output)  # [batch_size, 9]\n",
        "\n",
        "        # Pass through expert networks\n",
        "        expert_outputs = [expert(bert_output) for expert in self.experts]\n",
        "        expert_outputs = torch.stack(expert_outputs, dim=1)  # [batch_size, 9, 128]\n",
        "\n",
        "        # Apply domain gate weights\n",
        "        alpha = self.domain_gate(soft_labels).unsqueeze(-1)  # [batch_size, 9, 1]\n",
        "        v = (expert_outputs * alpha).sum(dim=1)  # Weighted sum of expert outputs\n",
        "\n",
        "        # Classification\n",
        "        y_hat = self.classifier(v)\n",
        "        return y_hat  # [batch_size]\n",
        "\n",
        "# --- Training and Evaluation ---\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=2e-5):\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss, total_acc = 0, 0\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = outputs.round().detach().cpu().numpy()\n",
        "            acc = accuracy_score(labels.cpu().numpy(), preds)\n",
        "            total_acc += acc\n",
        "\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}, Acc: {total_acc/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
        "\n",
        "def evaluate_model(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_acc = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = outputs.round().detach().cpu().numpy()\n",
        "            acc = accuracy_score(labels.cpu().numpy(), preds)\n",
        "            total_acc += acc\n",
        "\n",
        "    return total_loss / len(val_loader), total_acc / len(val_loader)\n",
        "\n",
        "# --- Example Usage ---\n",
        "file_path = '/content/Fake_Real_Data.csv'\n",
        "df = load_csv(file_path)\n",
        "\n",
        "texts, labels = preprocess_data(df, 'Text', 'label')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "max_len = 170\n",
        "\n",
        "# Splitting the data into training, validation, and test sets\n",
        "train_texts, train_labels = texts[:int(0.8*len(texts))], labels[:int(0.8*len(labels))]\n",
        "val_texts, val_labels = texts[int(0.8*len(texts)):int(0.9*len(texts))], labels[int(0.8*len(labels)):int(0.9*len(labels))]\n",
        "test_texts, test_labels = texts[int(0.9*len(texts)):], labels[int(0.9*len(labels)):]\n",
        "\n",
        "train_dataset = NewsDataset(train_texts, train_labels, tokenizer, max_len)\n",
        "val_dataset = NewsDataset(val_texts, val_labels, tokenizer, max_len)\n",
        "test_dataset = NewsDataset(test_texts, test_labels, tokenizer, max_len)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Initialize and train the model\n",
        "model = SLFENDModel()\n",
        "train_model(model, train_loader, val_loader)\n"
      ]
    }
  ]
}